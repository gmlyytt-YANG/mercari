{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from fastcache import clru_cache as lru_cache\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import os\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正则匹配\n",
    "whitespace = re.compile(r'\\s+')\n",
    "non_letter = re.compile(r'\\W+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "@lru_cache(1024)\n",
    "def stem(s):\n",
    "    return stemmer.stem(s)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"提取词干\"\"\"\n",
    "    text = text.lower()\n",
    "    text = non_letter.sub(' ', text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for t in text.split():\n",
    "        t = stem(t)\n",
    "        tokens.append(t)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths(tokens):\n",
    "    all_paths = ['/'.join(tokens[0:(i+1)]) for i in range(len(tokens))]\n",
    "    return ' '.join(all_paths)\n",
    "\n",
    "@lru_cache(1024)\n",
    "def cat_process(cat):\n",
    "    \"\"\"category不同层级类目串联\n",
    "    \n",
    "    eg: 输入 a/b/c， 输出 \"a a/b a/b/c\"\n",
    "    \"\"\"\n",
    "    cat = cat.lower()\n",
    "    cat = whitespace.sub('', cat)\n",
    "    split = cat.split('/')\n",
    "\n",
    "    return paths(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, min_df=10, tokenizer=str.split):\n",
    "        self.min_df = min_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.doc_freq = None\n",
    "        self.vocab = None\n",
    "        self.vocab_idx = None\n",
    "        self.max_len = None\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\"输入sentence数组，输出二维数组\"\"\"\n",
    "        tokenized = []\n",
    "        doc_freq = Counter()\n",
    "        n = len(texts)\n",
    "\n",
    "        # 收集sentence list中的词语频次\n",
    "        for text in texts:\n",
    "            sentence = self.tokenizer(text)\n",
    "            tokenized.append(sentence)\n",
    "            doc_freq.update(set(sentence))\n",
    "\n",
    "        # 生成词表\n",
    "        vocab = sorted([t for (t, c) in doc_freq.items() if c >= self.min_df])\n",
    "        vocab_idx = {t: (i + 1) for (i, t) in enumerate(vocab)}\n",
    "        doc_freq = [doc_freq[t] for t in vocab]\n",
    "\n",
    "        self.doc_freq = doc_freq\n",
    "        self.vocab = vocab\n",
    "        self.vocab_idx = vocab_idx\n",
    "\n",
    "        # 将sentence list中的元素转换为数字\n",
    "        max_len = 0\n",
    "        result_list = []\n",
    "        for text in tokenized:\n",
    "            text = self.text_to_idx(text)\n",
    "            max_len = max(max_len, len(text))\n",
    "            result_list.append(text)\n",
    "\n",
    "        # 每一个一维数组等长的二维数组\n",
    "        self.max_len = max_len\n",
    "        result = np.zeros(shape=(n, max_len), dtype=np.int32)\n",
    "        for i in range(n):\n",
    "            text = result_list[i]\n",
    "            result[i, :len(text)] = text\n",
    "\n",
    "        return result    \n",
    "\n",
    "    def text_to_idx(self, tokenized):\n",
    "        return [self.vocab_idx[t] for t in tokenized if t in self.vocab_idx]\n",
    "\n",
    "    def transform(self, texts):\n",
    "        \"\"\"输出类似fit_transform的二维数组\"\"\"\n",
    "        n = len(texts)\n",
    "        result = np.zeros(shape=(n, self.max_len), dtype=np.int32)\n",
    "\n",
    "        count = 0\n",
    "        for text_raw in texts:\n",
    "            text = self.tokenizer(text_raw)\n",
    "            text = self.text_to_idx(text)[:self.max_len]\n",
    "            result[count, :len(text)] = text\n",
    "            count += 1\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def vocabulary_size(self):\n",
    "        return len(self.vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "len(df_train) is 1037162\n",
      "len(df_valid) is 296332\n",
      "len(df_test) is 148167\n"
     ]
    }
   ],
   "source": [
    "print('reading data...')\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.2\n",
    "test_ratio = 0.1 # 暴露test数据集，方便后续拓展\n",
    "\n",
    "df = pd.read_csv('../input/train.tsv', sep='\\t')\n",
    "df = df[df.price != 0].reset_index(drop=True)\n",
    "df_index = df.index\n",
    "\n",
    "# train\n",
    "df_train = df.sample(int(train_ratio * len(df)))\n",
    "train_index = df_train.index\n",
    "print(\"len(df_train) is {}\".format(len(df_train)))\n",
    "\n",
    "# minus train\n",
    "residue_index = df_index.difference(train_index)\n",
    "df_residue = df.loc[residue_index]\n",
    "\n",
    "# valid\n",
    "valid_ratio_in_residue = float(valid_ratio) / (valid_ratio + test_ratio)\n",
    "df_valid = df_residue.sample(int(valid_ratio_in_residue * len(df_residue)))\n",
    "valid_index = df_valid.index\n",
    "print(\"len(df_valid) is {}\".format(len(df_valid)))\n",
    "\n",
    "# test\n",
    "test_index = residue_index.difference(valid_index)\n",
    "df_test = df_residue.loc[test_index]\n",
    "print(\"len(df_test) is {}\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    \"\"\"生成label\"\"\"\n",
    "    price = df.pop('price')\n",
    "    \n",
    "    # 平滑处理\n",
    "    y = np.log1p(price.values)\n",
    "    y = (y - y.mean()) / y.std()\n",
    "    \n",
    "    return y.reshape(-1, 1)\n",
    "\n",
    "y_train = get_label(df_train)\n",
    "y_valid = get_label(df_valid)\n",
    "y_test = get_label(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.name.fillna('unkname', inplace=True)\n",
    "df_train.category_name.fillna('unk_cat', inplace=True)\n",
    "df_train.brand_name.fillna('unk_brand', inplace=True)\n",
    "df_train.item_description.fillna('nodesc', inplace=True)\n",
    "\n",
    "df_valid.name.fillna('unkname', inplace=True)\n",
    "df_valid.category_name.fillna('unk_cat', inplace=True)\n",
    "df_valid.brand_name.fillna('unk_brand', inplace=True)\n",
    "df_valid.item_description.fillna('nodesc', inplace=True)\n",
    "\n",
    "df_test.name.fillna('unkname', inplace=True)\n",
    "df_test.category_name.fillna('unk_cat', inplace=True)\n",
    "df_test.brand_name.fillna('unk_brand', inplace=True)\n",
    "df_test.item_description.fillna('nodesc', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1423464</th>\n",
       "      <td>1424303</td>\n",
       "      <td>New never used bullet lighter</td>\n",
       "      <td>1</td>\n",
       "      <td>Men/Other/Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Perfect for any man's Xmas. Never even been fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018359</th>\n",
       "      <td>1018938</td>\n",
       "      <td>Senior Shirts</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Tops &amp; Blouses/T-Shirts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2 class of 2016 shirts Never been opened Size ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841688</th>\n",
       "      <td>842149</td>\n",
       "      <td>Calvin Klein</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Coats &amp; Jackets/Trench</td>\n",
       "      <td>Calvin Klein</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Trench style coat by Ck black size 12...worn o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246122</th>\n",
       "      <td>246248</td>\n",
       "      <td>HOLD B &amp; W Julia Dress Only</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Dresses/Knee-Length</td>\n",
       "      <td>LuLaRoe</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>the tag is cut out bc my daughter is weird abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125542</th>\n",
       "      <td>1126186</td>\n",
       "      <td>Men's Levi's 569</td>\n",
       "      <td>2</td>\n",
       "      <td>Men/Jeans/Classic, Straight Leg</td>\n",
       "      <td>Levi's®</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Size 32 x 32. Brand new, never worn. No tags. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_id                           name  item_condition_id  \\\n",
       "1423464   1424303  New never used bullet lighter                  1   \n",
       "1018359   1018938                  Senior Shirts                  1   \n",
       "841688     842149                   Calvin Klein                  3   \n",
       "246122     246248    HOLD B & W Julia Dress Only                  3   \n",
       "1125542   1126186               Men's Levi's 569                  2   \n",
       "\n",
       "                           category_name    brand_name  price  shipping  \\\n",
       "1423464                  Men/Other/Other           NaN    7.0         1   \n",
       "1018359    Women/Tops & Blouses/T-Shirts           NaN    7.0         0   \n",
       "841688      Women/Coats & Jackets/Trench  Calvin Klein   19.0         0   \n",
       "246122         Women/Dresses/Knee-Length       LuLaRoe   21.0         0   \n",
       "1125542  Men/Jeans/Classic, Straight Leg       Levi's®   20.0         1   \n",
       "\n",
       "                                          item_description  \n",
       "1423464  Perfect for any man's Xmas. Never even been fi...  \n",
       "1018359  2 class of 2016 shirts Never been opened Size ...  \n",
       "841688   Trench style coat by Ck black size 12...worn o...  \n",
       "246122   the tag is cut out bc my daughter is weird abo...  \n",
       "1125542  Size 32 x 32. Brand new, never worn. No tags. ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing category...\n"
     ]
    }
   ],
   "source": [
    "# 每一个category、title、description都可以类比成一个句子，因而需要用Tokenizer生成二维数组\n",
    "print('processing category...')\n",
    "\n",
    "cat_tok = Tokenizer(min_df=50)\n",
    "\n",
    "# 不同类目下的相同词语代表的含义不同比如nike/fashionsneakers和shoes/fashionsneakers的含义不同\n",
    "df_train_category_name = df_train.category_name.apply(cat_process)\n",
    "X_cat = cat_tok.fit_transform(df_train_category_name)\n",
    "cat_voc_size = cat_tok.vocabulary_size()\n",
    "\n",
    "df_valid_category_name = df_valid.category_name.apply(cat_process)\n",
    "X_cat_valid = cat_tok.transform(df_valid_category_name)\n",
    "\n",
    "df_test_category_name = df_test.category_name.apply(cat_process)\n",
    "X_cat_test = cat_tok.transform(df_test_category_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing title...\n"
     ]
    }
   ],
   "source": [
    "print('processing title...')\n",
    "\n",
    "name_tok = Tokenizer(min_df=10, tokenizer=tokenize)\n",
    "X_name = name_tok.fit_transform(df_train.name)\n",
    "name_voc_size = name_tok.vocabulary_size()\n",
    "\n",
    "X_name_valid = name_tok.transform(df_valid.name)\n",
    "X_name_test = name_tok.transform(df_test.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing description...\n"
     ]
    }
   ],
   "source": [
    "print('processing description...')\n",
    "\n",
    "desc_num_col = 40\n",
    "desc_tok = Tokenizer(min_df=50, tokenizer=tokenize)\n",
    "\n",
    "X_desc = desc_tok.fit_transform(df_train.item_description)\n",
    "X_desc_valid = desc_tok.transform(df_valid.item_description)\n",
    "X_desc_test = desc_tok.transform(df_test.item_description)\n",
    "\n",
    "X_desc = X_desc[:, :desc_num_col] # 截断长尾特征\n",
    "X_desc_valid = X_desc_valid[:, :desc_num_col]\n",
    "X_desc_test = X_desc_test[:, :desc_num_col]\n",
    "\n",
    "desc_voc_size = desc_tok.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing brand...\n"
     ]
    }
   ],
   "source": [
    "print('processing brand...')\n",
    "\n",
    "df_train.brand_name = df_train.brand_name.str.lower().replace(' ', '_')\n",
    "df_valid.brand_name = df_valid.brand_name.str.lower().replace(' ', '_')\n",
    "df_test.brand_name = df_test.brand_name.str.lower().replace(' ', '_')\n",
    "\n",
    "# 生成词表\n",
    "brand_cnt = Counter(df_train.brand_name[df_train.brand_name != 'unk_brand'])\n",
    "brands = sorted(b for (b, c) in brand_cnt.items() if c >= 50) # 词表\n",
    "brands_idx = {b: (i + 1) for (i, b) in enumerate(brands)} # 单词编号\n",
    "\n",
    "X_brand = df_train.brand_name.apply(lambda b: brands_idx.get(b, 0))\n",
    "X_brand_valid = df_valid.brand_name.apply(lambda b: brands_idx.get(b, 0))\n",
    "X_brand_test = df_test.brand_name.apply(lambda b: brands_idx.get(b, 0))\n",
    "\n",
    "X_brand = X_brand.values.reshape(-1, 1) \n",
    "X_brand_valid = X_brand_valid.values.reshape(-1, 1) \n",
    "X_brand_test = X_brand_test.values.reshape(-1, 1)\n",
    "\n",
    "brand_voc_size = len(brands) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing other features...\n"
     ]
    }
   ],
   "source": [
    "print('processing other features...')\n",
    "\n",
    "X_item_cond = (df_train.item_condition_id - 1).astype('uint8').values.reshape(-1, 1)\n",
    "X_item_cond_valid = (df_valid.item_condition_id - 1).astype('uint8').values.reshape(-1, 1)\n",
    "X_item_cond_test = (df_test.item_condition_id - 1).astype('uint8').values.reshape(-1, 1)\n",
    "\n",
    "X_shipping = df_train.shipping.astype('float32').values.reshape(-1, 1)\n",
    "X_shipping_valid = df_valid.shipping.astype('float32').values.reshape(-1, 1)\n",
    "X_shipping_test = df_test.shipping.astype('float32').values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(inputs, num_filters, filter_size, padding='same'):\n",
    "    he_std = np.sqrt(2 / (filter_size * num_filters))\n",
    "    out = tf.layers.conv1d(\n",
    "        inputs=inputs, filters=num_filters, padding=padding,\n",
    "        kernel_size=filter_size,\n",
    "        activation=tf.nn.relu, \n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=he_std))\n",
    "    return out\n",
    "\n",
    "def dense(X, size, reg=0.0, activation=None):\n",
    "    he_std = np.sqrt(2 / int(X.shape[1]))\n",
    "    out = tf.layers.dense(X, units=size, activation=activation, \n",
    "                     kernel_initializer=tf.random_normal_initializer(stddev=he_std),\n",
    "                     kernel_regularizer=tf.contrib.layers.l2_regularizer(reg))\n",
    "    return out\n",
    "\n",
    "def embed(inputs, size, dim):\n",
    "    \"\"\"输出[sie, dim]中经过inputs过滤的embedding向量\"\"\"\n",
    "    std = np.sqrt(2 / dim)\n",
    "    emb = tf.Variable(tf.random_uniform([size, dim], -std, std))\n",
    "    lookup = tf.nn.embedding_lookup(emb, inputs)\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_embeddings_dim = 32\n",
    "name_seq_len = X_name.shape[1]\n",
    "\n",
    "desc_embeddings_dim = 32\n",
    "desc_seq_len = X_desc.shape[1]\n",
    "\n",
    "brand_embeddings_dim = 4\n",
    "\n",
    "cat_embeddings_dim = 12\n",
    "cat_seq_len = X_cat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name.shape is (?, 130)\n",
      "desc.shape is (?, 400)\n",
      "brand.shape is (?, 4)\n",
      "cat.shape is (?, 12)\n",
      "concatenated dim: (?, 552)\n"
     ]
    }
   ],
   "source": [
    "# factor machines\n",
    "graph = tf.Graph()\n",
    "graph.seed = 1\n",
    "\n",
    "with graph.as_default():\n",
    "    place_name = tf.placeholder(tf.int32, shape=(None, name_seq_len))\n",
    "    place_desc = tf.placeholder(tf.int32, shape=(None, desc_seq_len))\n",
    "    place_brand = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    place_cat = tf.placeholder(tf.int32, shape=(None, cat_seq_len))\n",
    "    place_ship = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    place_cond = tf.placeholder(tf.uint8, shape=(None, 1))\n",
    "    place_y = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "    place_lr = tf.placeholder(tf.float32, shape=(), )\n",
    "    \n",
    "    # title\n",
    "    name = embed(place_name, name_voc_size, name_embeddings_dim)\n",
    "    name = conv1d(name, num_filters=10, filter_size=3)\n",
    "    name = tf.nn.dropout(name, keep_prob=0.5)\n",
    "    name = tf.layers.flatten(name)\n",
    "    tf.summary.histogram(\"name\", name)\n",
    "    print(\"name.shape is {}\".format(name.shape))\n",
    "    \n",
    "    # description \n",
    "    desc = embed(place_desc, desc_voc_size, desc_embeddings_dim)\n",
    "    desc = conv1d(desc, num_filters=10, filter_size=3)\n",
    "    desc = tf.nn.dropout(desc, keep_prob=0.5)\n",
    "    desc = tf.layers.flatten(desc)\n",
    "    tf.summary.histogram(\"desc\", desc)\n",
    "    print(\"desc.shape is {}\".format(desc.shape))\n",
    "    \n",
    "    # brand\n",
    "    brand = embed(place_brand, brand_voc_size, brand_embeddings_dim)\n",
    "    brand = tf.layers.flatten(brand)\n",
    "    tf.summary.histogram(\"brand\", brand)\n",
    "    print(\"brand.shape is {}\".format(brand.shape))\n",
    "    \n",
    "    # category \n",
    "    cat = embed(place_cat, cat_voc_size, cat_embeddings_dim)\n",
    "    cat = tf.layers.average_pooling1d(cat, pool_size=cat_seq_len, strides=1, padding='valid')\n",
    "    cat = tf.layers.flatten(cat)\n",
    "    tf.summary.histogram(\"cat\", cat)\n",
    "    print(\"cat.shape is {}\".format(cat.shape))\n",
    "    \n",
    "    # ship\n",
    "    ship = place_ship\n",
    "\n",
    "    # condition\n",
    "    cond = tf.one_hot(place_cond, 5)\n",
    "    cond = tf.layers.flatten(cond)\n",
    "\n",
    "    out = tf.concat([name, desc, brand, cat, ship, cond], axis=1)\n",
    "    print('concatenated dim:', out.shape)\n",
    "    \n",
    "    # out = dense(out, size=100, activation=None)\n",
    "    # out = tf.nn.dropout(out, keep_prob=0.5)\n",
    "    # out = dense(out, size=1)\n",
    "    \n",
    "    w_0 = tf.Variable(tf.random_normal([1], stddev=0.01))\n",
    "    W = tf.Variable(tf.random_normal([out.shape[1].value, 1], stddev=0.01))\n",
    "    linear_out = tf.add(tf.matmul(out, W), w_0)\n",
    "    \n",
    "    k = 5\n",
    "    V = tf.Variable(tf.random_normal([out.shape[1].value, k], stddev=0.01))\n",
    "    complex_output = tf.multiply(tf.reduce_sum(tf.subtract(tf.pow(tf.matmul(out, V), 2), \\\n",
    "                                                tf.matmul(tf.pow(out, 2), tf.pow(V, 2))), \\\n",
    "                                    axis=1, keep_dims=True), 0.5)\n",
    "    \n",
    "    out_y = tf.add(linear_out, complex_output)\n",
    "    lambda_w = tf.constant(0.001, dtype=tf.float32)\n",
    "    lambda_v = tf.constant(0.001, dtype=tf.float32)\n",
    "    regularization = tf.reduce_sum(tf.multiply(lambda_w, tf.pow(W, 2))) + \\\n",
    "            tf.reduce_sum(tf.multiply(lambda_v, tf.pow(V, 2)))\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(place_y, out_y) + regularization\n",
    "    tf.summary.scalar(\"loss\", loss) \n",
    "    rmse = tf.sqrt(loss)\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=place_lr).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    merged = tf.summary.merge_all() # merge_all需要在graph的定义中声明，否则无效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(inputs, batch_size):\n",
    "    \"\"\"提供batch_size数据\"\"\"\n",
    "    n = len(inputs)\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield inputs[i: i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 300, loss: 0.5835241079330444\n",
      "iter: 600, loss: 0.5017719864845276\n",
      "iter: 900, loss: 0.47363460063934326\n",
      "iter: 1200, loss: 0.45736998319625854\n",
      "iter: 1500, loss: 0.443903386592865\n",
      "iter: 1800, loss: 0.4387122094631195\n",
      "iter: 2100, loss: 0.4315462112426758\n",
      "iter: 2400, loss: 0.42835551500320435\n",
      "iter: 2700, loss: 0.42728134989738464\n",
      "iter: 3000, loss: 0.4229772388935089\n",
      "iter: 3300, loss: 0.42150256037712097\n",
      "iter: 3600, loss: 0.41808849573135376\n",
      "iter: 3900, loss: 0.417599618434906\n",
      "iter: 4200, loss: 0.41644126176834106\n",
      "iter: 4500, loss: 0.41566333174705505\n",
      "iter: 4800, loss: 0.4137185215950012\n",
      "iter: 5100, loss: 0.4118919372558594\n",
      "iter: 5400, loss: 0.4120136499404907\n",
      "iter: 5700, loss: 0.41022777557373047\n",
      "iter: 6000, loss: 0.4082210063934326\n",
      "iter: 6300, loss: 0.40746423602104187\n",
      "iter: 6600, loss: 0.40592241287231445\n",
      "iter: 6900, loss: 0.4056656062602997\n",
      "iter: 7200, loss: 0.4044383466243744\n",
      "iter: 7500, loss: 0.40455275774002075\n",
      "iter: 7800, loss: 0.4036361873149872\n",
      "iter: 8100, loss: 0.4031221866607666\n",
      "iter: 8400, loss: 0.4030923843383789\n",
      "iter: 8700, loss: 0.4022739827632904\n",
      "iter: 9000, loss: 0.4026300311088562\n",
      "iter: 9300, loss: 0.4029669165611267\n",
      "iter: 9600, loss: 0.4026302397251129\n",
      "iter: 9900, loss: 0.403403639793396\n",
      "iter: 10200, loss: 0.40204253792762756\n",
      "iter: 10500, loss: 0.40183767676353455\n",
      "iter: 10800, loss: 0.4027310907840729\n",
      "iter: 11100, loss: 0.4021555185317993\n",
      "iter: 11400, loss: 0.40183353424072266\n",
      "iter: 11700, loss: 0.4013913869857788\n",
      "iter: 12000, loss: 0.4018833041191101\n",
      "iter: 12300, loss: 0.4015122354030609\n",
      "iter: 12600, loss: 0.4012812674045563\n",
      "iter: 12900, loss: 0.4007366895675659\n",
      "iter: 13200, loss: 0.4005516469478607\n",
      "iter: 13500, loss: 0.40093791484832764\n",
      "iter: 13800, loss: 0.40162765979766846\n",
      "iter: 14100, loss: 0.4002670347690582\n",
      "iter: 14400, loss: 0.4006052017211914\n",
      "iter: 14700, loss: 0.4004955291748047\n",
      "iter: 15000, loss: 0.4009045362472534\n",
      "iter: 15300, loss: 0.40066954493522644\n",
      "iter: 15600, loss: 0.40048936009407043\n",
      "iter: 15900, loss: 0.4001057744026184\n",
      "iter: 16200, loss: 0.40031445026397705\n",
      "iter: 16500, loss: 0.39958393573760986\n",
      "iter: 16800, loss: 0.40069085359573364\n",
      "iter: 17100, loss: 0.39979588985443115\n",
      "iter: 17400, loss: 0.400193989276886\n",
      "iter: 17700, loss: 0.4011734127998352\n",
      "iter: 18000, loss: 0.40020254254341125\n",
      "iter: 18300, loss: 0.3996271789073944\n",
      "iter: 18600, loss: 0.3993504047393799\n",
      "iter: 18900, loss: 0.40000778436660767\n",
      "iter: 19200, loss: 0.3997599482536316\n",
      "iter: 19500, loss: 0.40008464455604553\n",
      "iter: 19800, loss: 0.3996843099594116\n",
      "iter: 20100, loss: 0.39990031719207764\n",
      "iter: 20400, loss: 0.39974793791770935\n",
      "iter: 20700, loss: 0.3992959260940552\n",
      "iter: 21000, loss: 0.3997269570827484\n",
      "iter: 21300, loss: 0.3994612395763397\n",
      "iter: 21600, loss: 0.3999633193016052\n",
      "iter: 21900, loss: 0.39958369731903076\n",
      "iter: 22200, loss: 0.3985825181007385\n",
      "iter: 22500, loss: 0.3991028964519501\n",
      "iter: 22800, loss: 0.3992273211479187\n",
      "iter: 23100, loss: 0.39822158217430115\n",
      "iter: 23400, loss: 0.39882469177246094\n",
      "iter: 23700, loss: 0.3990321159362793\n",
      "iter: 24000, loss: 0.39883914589881897\n",
      "iter: 24300, loss: 0.3993719816207886\n",
      "iter: 24600, loss: 0.3990323543548584\n",
      "iter: 24900, loss: 0.39901208877563477\n",
      "iter: 25200, loss: 0.3988266587257385\n",
      "iter: 25500, loss: 0.3993318974971771\n",
      "iter: 25800, loss: 0.3981855809688568\n",
      "iter: 26100, loss: 0.3983834981918335\n",
      "iter: 26400, loss: 0.39880046248435974\n",
      "iter: 26700, loss: 0.3985742926597595\n",
      "iter: 27000, loss: 0.39889034628868103\n",
      "iter: 27300, loss: 0.3983692228794098\n",
      "iter: 27600, loss: 0.3988910913467407\n",
      "iter: 27900, loss: 0.3986940383911133\n",
      "iter: 28200, loss: 0.39865559339523315\n",
      "iter: 28500, loss: 0.3984408974647522\n",
      "iter: 28800, loss: 0.3982931971549988\n",
      "iter: 29100, loss: 0.39875492453575134\n",
      "iter: 29400, loss: 0.3982807397842407\n",
      "iter: 29700, loss: 0.39889928698539734\n",
      "iter: 30000, loss: 0.39851611852645874\n",
      "iter: 30300, loss: 0.39854666590690613\n",
      "iter: 30600, loss: 0.3981345295906067\n",
      "iter: 30900, loss: 0.3984343111515045\n",
      "iter: 31200, loss: 0.3983083963394165\n",
      "iter: 31500, loss: 0.39844802021980286\n",
      "iter: 31800, loss: 0.39829444885253906\n",
      "iter: 32100, loss: 0.39878523349761963\n",
      "iter: 32400, loss: 0.3979213833808899\n",
      "iter: 32700, loss: 0.39917489886283875\n",
      "iter: 33000, loss: 0.39812248945236206\n",
      "iter: 33300, loss: 0.39802226424217224\n",
      "iter: 33600, loss: 0.39854252338409424\n",
      "iter: 33900, loss: 0.39878198504447937\n",
      "iter: 34200, loss: 0.39840197563171387\n",
      "iter: 34500, loss: 0.3981108069419861\n",
      "iter: 34800, loss: 0.39766189455986023\n",
      "iter: 35100, loss: 0.39813482761383057\n",
      "iter: 35400, loss: 0.3988431990146637\n",
      "iter: 35700, loss: 0.398211270570755\n",
      "iter: 36000, loss: 0.39796891808509827\n",
      "iter: 36300, loss: 0.3973503112792969\n",
      "iter: 36600, loss: 0.39810216426849365\n",
      "iter: 36900, loss: 0.3976094126701355\n",
      "iter: 37200, loss: 0.3981725573539734\n",
      "iter: 37500, loss: 0.3976070284843445\n",
      "iter: 37800, loss: 0.3977292478084564\n",
      "iter: 38100, loss: 0.3985269069671631\n",
      "iter: 38400, loss: 0.3971877694129944\n",
      "iter: 38700, loss: 0.39819926023483276\n",
      "iter: 39000, loss: 0.3978367745876312\n",
      "iter: 39300, loss: 0.39763563871383667\n",
      "iter: 39600, loss: 0.3985283672809601\n",
      "iter: 39900, loss: 0.396968275308609\n",
      "iter: 40200, loss: 0.39779549837112427\n",
      "iter: 40500, loss: 0.3979751765727997\n",
      "iter: 40800, loss: 0.3977966904640198\n",
      "iter: 41100, loss: 0.39837953448295593\n",
      "iter: 41400, loss: 0.39793896675109863\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session(graph=graph)\n",
    "writer = tf.summary.FileWriter(\"loss_log/\", session.graph)\n",
    "session.run(init)\n",
    "\n",
    "count = 0\n",
    "for i in range(20):\n",
    "    np.random.seed(i)\n",
    "    train_indices = list(range(len(X_name)))\n",
    "    np.random.shuffle(train_indices)\n",
    "    \n",
    "    # 控制lr \n",
    "    if i <= 2:\n",
    "        lr = 0.001\n",
    "    elif i <= 10:\n",
    "        lr = 0.0001\n",
    "    else:\n",
    "        lr = 0.00005\n",
    "    \n",
    "    for idx in prepare_batches(train_indices, 500):\n",
    "        feed_dict_op = {\n",
    "            place_name: X_name[idx],\n",
    "            place_desc: X_desc[idx],\n",
    "            place_brand: X_brand[idx],\n",
    "            place_cat: X_cat[idx],\n",
    "            place_ship: X_shipping[idx],\n",
    "            place_cond: X_item_cond[idx],\n",
    "            place_y: y_train[idx],\n",
    "            place_lr: lr\n",
    "        }\n",
    "        \n",
    "        _ = session.run(train_step, feed_dict=feed_dict_op)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 300 == 0:\n",
    "            feed_dict_op = {\n",
    "                place_name: X_name_valid,\n",
    "                place_desc: X_desc_valid,\n",
    "                place_brand: X_brand_valid,\n",
    "                place_cat: X_cat_valid,\n",
    "                place_ship: X_shipping_valid,\n",
    "                place_cond: X_item_cond_valid,\n",
    "                place_y: y_valid\n",
    "            }\n",
    "            loss_op, merged_op = session.run([loss, merged], feed_dict_op)\n",
    "            print(\"iter: {}, loss: {}\".format(count, loss_op))\n",
    "            writer.add_summary(merged_op, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
