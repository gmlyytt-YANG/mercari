{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from fastcache import clru_cache as lru_cache\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import os\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正则匹配\n",
    "whitespace = re.compile(r'\\s+')\n",
    "non_letter = re.compile(r'\\W+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "@lru_cache(1024)\n",
    "def stem(s):\n",
    "    return stemmer.stem(s)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"提取词干\"\"\"\n",
    "    text = text.lower()\n",
    "    text = non_letter.sub(' ', text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    for t in text.split():\n",
    "        t = stem(t)\n",
    "        tokens.append(t)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths(tokens):\n",
    "    all_paths = ['/'.join(tokens[0:(i+1)]) for i in range(len(tokens))]\n",
    "    return ' '.join(all_paths)\n",
    "\n",
    "@lru_cache(1024)\n",
    "def cat_process(cat):\n",
    "    \"\"\"category不同层级类目串联\n",
    "    \n",
    "    eg: 输入 a/b/c， 输出 \"a a/b a/b/c\"\n",
    "    \"\"\"\n",
    "    cat = cat.lower()\n",
    "    cat = whitespace.sub('', cat)\n",
    "    split = cat.split('/')\n",
    "\n",
    "    return paths(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, min_df=10, tokenizer=str.split):\n",
    "        self.min_df = min_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.doc_freq = None\n",
    "        self.vocab = None\n",
    "        self.vocab_idx = None\n",
    "        self.max_len = None\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\"输入sentence数组，输出二维数组\"\"\"\n",
    "        tokenized = []\n",
    "        doc_freq = Counter()\n",
    "        n = len(texts)\n",
    "\n",
    "        # 收集sentence list中的词语频次\n",
    "        for text in texts:\n",
    "            sentence = self.tokenizer(text)\n",
    "            tokenized.append(sentence)\n",
    "            doc_freq.update(set(sentence))\n",
    "\n",
    "        # 生成词表\n",
    "        vocab = sorted([t for (t, c) in doc_freq.items() if c >= self.min_df])\n",
    "        vocab_idx = {t: (i + 1) for (i, t) in enumerate(vocab)}\n",
    "        doc_freq = [doc_freq[t] for t in vocab]\n",
    "\n",
    "        self.doc_freq = doc_freq\n",
    "        self.vocab = vocab\n",
    "        self.vocab_idx = vocab_idx\n",
    "\n",
    "        # 将sentence list中的元素转换为数字\n",
    "        max_len = 0\n",
    "        result_list = []\n",
    "        for text in tokenized:\n",
    "            text = self.text_to_idx(text)\n",
    "            max_len = max(max_len, len(text))\n",
    "            result_list.append(text)\n",
    "\n",
    "        # 每一个一维数组等长的二维数组\n",
    "        self.max_len = max_len\n",
    "        result = np.zeros(shape=(n, max_len), dtype=np.int32)\n",
    "        for i in range(n):\n",
    "            text = result_list[i]\n",
    "            result[i, :len(text)] = text\n",
    "\n",
    "        return result    \n",
    "\n",
    "    def text_to_idx(self, tokenized):\n",
    "        return [self.vocab_idx[t] for t in tokenized if t in self.vocab_idx]\n",
    "\n",
    "    def transform(self, texts):\n",
    "        \"\"\"输出类似fit_transform的二维数组\"\"\"\n",
    "        n = len(texts)\n",
    "        result = np.zeros(shape=(n, self.max_len), dtype=np.int32)\n",
    "\n",
    "        count = 0\n",
    "        for text_raw in texts:\n",
    "            text = self.tokenizer(text_raw)\n",
    "            text = self.text_to_idx(text)[:self.max_len]\n",
    "            result[count, :len(text)] = text\n",
    "            count += 1\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def vocabulary_size(self):\n",
    "        return len(self.vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "len(df_train) is 1037162\n",
      "len(df_valid) is 296332\n",
      "len(df_test) is 148167\n"
     ]
    }
   ],
   "source": [
    "print('reading data...')\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.2\n",
    "test_ratio = 0.1 # 暴露test数据集，方便后续拓展\n",
    "\n",
    "df = pd.read_csv('../input/train.tsv', sep='\\t')\n",
    "df = df[df.price != 0].reset_index(drop=True)\n",
    "df_index = df.index\n",
    "\n",
    "# train\n",
    "df_train = df.sample(int(train_ratio * len(df)))\n",
    "train_index = df_train.index\n",
    "print(\"len(df_train) is {}\".format(len(df_train)))\n",
    "\n",
    "# minus train\n",
    "residue_index = df_index.difference(train_index)\n",
    "df_residue = df.loc[residue_index]\n",
    "\n",
    "# valid\n",
    "valid_ratio_in_residue = float(valid_ratio) / (valid_ratio + test_ratio)\n",
    "df_valid = df_residue.sample(int(valid_ratio_in_residue * len(df_residue)))\n",
    "valid_index = df_valid.index\n",
    "print(\"len(df_valid) is {}\".format(len(df_valid)))\n",
    "\n",
    "# test\n",
    "test_index = residue_index.difference(valid_index)\n",
    "df_test = df_residue.loc[test_index]\n",
    "print(\"len(df_test) is {}\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    \"\"\"生成label\"\"\"\n",
    "    price = df.pop('price')\n",
    "    \n",
    "    # 平滑处理\n",
    "    y = np.log1p(price.values)\n",
    "    y = (y - y.mean()) / y.std()\n",
    "    \n",
    "    return y.reshape(-1, 1)\n",
    "\n",
    "y_train = get_label(df_train)\n",
    "y_valid = get_label(df_valid)\n",
    "y_test = get_label(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.name.fillna('unkname', inplace=True)\n",
    "df_train.category_name.fillna('unk_cat', inplace=True)\n",
    "df_train.brand_name.fillna('unk_brand', inplace=True)\n",
    "df_train.item_description.fillna('nodesc', inplace=True)\n",
    "\n",
    "df_valid.name.fillna('unkname', inplace=True)\n",
    "df_valid.category_name.fillna('unk_cat', inplace=True)\n",
    "df_valid.brand_name.fillna('unk_brand', inplace=True)\n",
    "df_valid.item_description.fillna('nodesc', inplace=True)\n",
    "\n",
    "df_test.name.fillna('unkname', inplace=True)\n",
    "df_test.category_name.fillna('unk_cat', inplace=True)\n",
    "df_test.brand_name.fillna('unk_brand', inplace=True)\n",
    "df_test.item_description.fillna('nodesc', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>592471</th>\n",
       "      <td>592781</td>\n",
       "      <td>Size 2 Womens Bundle</td>\n",
       "      <td>2</td>\n",
       "      <td>Women/Pants/Dress Pants</td>\n",
       "      <td>old navy</td>\n",
       "      <td>0</td>\n",
       "      <td>1 pair of capris- Rockies Jeans Brand. Midrise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881053</th>\n",
       "      <td>881541</td>\n",
       "      <td>Federal bowl</td>\n",
       "      <td>3</td>\n",
       "      <td>Vintage &amp; Collectibles/Housewares/Bowl</td>\n",
       "      <td>unk_brand</td>\n",
       "      <td>0</td>\n",
       "      <td>Vintage federal mixing bowl. milk glass yellow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376697</th>\n",
       "      <td>1377503</td>\n",
       "      <td>Victoria secret pink long sleeve</td>\n",
       "      <td>2</td>\n",
       "      <td>Women/Tops &amp; Blouses/T-Shirts</td>\n",
       "      <td>pink</td>\n",
       "      <td>1</td>\n",
       "      <td>No rips or tears Really soft lettering on it S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411685</th>\n",
       "      <td>1412514</td>\n",
       "      <td>H&amp;M basics bundle</td>\n",
       "      <td>2</td>\n",
       "      <td>Women/Tops &amp; Blouses/T-Shirts</td>\n",
       "      <td>h&amp;m</td>\n",
       "      <td>1</td>\n",
       "      <td>-free ship- Includes 2 basic tees from H&amp;M in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191720</th>\n",
       "      <td>1192406</td>\n",
       "      <td>Feminist Pin</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Women's Accessories/Other</td>\n",
       "      <td>unk_brand</td>\n",
       "      <td>1</td>\n",
       "      <td>Handmade. One of a kind button that reads \"I'm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_id                              name  item_condition_id  \\\n",
       "592471     592781              Size 2 Womens Bundle                  2   \n",
       "881053     881541                      Federal bowl                  3   \n",
       "1376697   1377503  Victoria secret pink long sleeve                  2   \n",
       "1411685   1412514                 H&M basics bundle                  2   \n",
       "1191720   1192406                      Feminist Pin                  1   \n",
       "\n",
       "                                  category_name brand_name  shipping  \\\n",
       "592471                  Women/Pants/Dress Pants   old navy         0   \n",
       "881053   Vintage & Collectibles/Housewares/Bowl  unk_brand         0   \n",
       "1376697           Women/Tops & Blouses/T-Shirts       pink         1   \n",
       "1411685           Women/Tops & Blouses/T-Shirts        h&m         1   \n",
       "1191720         Women/Women's Accessories/Other  unk_brand         1   \n",
       "\n",
       "                                          item_description  \n",
       "592471   1 pair of capris- Rockies Jeans Brand. Midrise...  \n",
       "881053   Vintage federal mixing bowl. milk glass yellow...  \n",
       "1376697  No rips or tears Really soft lettering on it S...  \n",
       "1411685  -free ship- Includes 2 basic tees from H&M in ...  \n",
       "1191720  Handmade. One of a kind button that reads \"I'm...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing category...\n"
     ]
    }
   ],
   "source": [
    "# 每一个category、title、description都可以类比成一个句子，因而需要用Tokenizer生成二维数组\n",
    "print('processing category...')\n",
    "\n",
    "cat_tok = Tokenizer(min_df=50)\n",
    "\n",
    "# 不同类目下的相同词语代表的含义不同比如nike/fashionsneakers和shoes/fashionsneakers的含义不同\n",
    "df_train_category_name = df_train.category_name.apply(cat_process)\n",
    "X_cat = cat_tok.fit_transform(df_train_category_name)\n",
    "cat_voc_size = cat_tok.vocabulary_size()\n",
    "\n",
    "df_valid_category_name = df_valid.category_name.apply(cat_process)\n",
    "X_cat_valid = cat_tok.transform(df_valid_category_name)\n",
    "\n",
    "df_test_category_name = df_test.category_name.apply(cat_process)\n",
    "X_cat_test = cat_tok.transform(df_test_category_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing title...\n"
     ]
    }
   ],
   "source": [
    "print('processing title...')\n",
    "\n",
    "name_tok = Tokenizer(min_df=10, tokenizer=tokenize)\n",
    "X_name = name_tok.fit_transform(df_train.name)\n",
    "name_voc_size = name_tok.vocabulary_size()\n",
    "\n",
    "X_name_valid = name_tok.transform(df_valid.name)\n",
    "X_name_test = name_tok.transform(df_test.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing description...\n"
     ]
    }
   ],
   "source": [
    "print('processing description...')\n",
    "\n",
    "desc_num_col = 40\n",
    "desc_tok = Tokenizer(min_df=50, tokenizer=tokenize)\n",
    "\n",
    "X_desc = desc_tok.fit_transform(df_train.item_description)\n",
    "X_desc_valid = desc_tok.transform(df_valid.item_description)\n",
    "X_desc_test = desc_tok.transform(df_test.item_description)\n",
    "\n",
    "X_desc = X_desc[:, :desc_num_col] # 截断长尾特征\n",
    "X_desc_valid = X_desc_valid[:, :desc_num_col]\n",
    "X_desc_test = X_desc_test[:, :desc_num_col]\n",
    "\n",
    "desc_voc_size = desc_tok.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing brand...\n"
     ]
    }
   ],
   "source": [
    "print('processing brand...')\n",
    "\n",
    "df_train.brand_name = df_train.brand_name.str.lower().replace(' ', '_')\n",
    "df_valid.brand_name = df_valid.brand_name.str.lower().replace(' ', '_')\n",
    "df_test.brand_name = df_test.brand_name.str.lower().replace(' ', '_')\n",
    "\n",
    "# 生成词表\n",
    "brand_cnt = Counter(df_train.brand_name[df_train.brand_name != 'unk_brand'])\n",
    "brands = sorted(b for (b, c) in brand_cnt.items() if c >= 50) # 词表\n",
    "brands_idx = {b: (i + 1) for (i, b) in enumerate(brands)} # 单词编号\n",
    "\n",
    "X_brand = df_train.brand_name.apply(lambda b: brands_idx.get(b, 0))\n",
    "X_brand_valid = df_valid.brand_name.apply(lambda b: brands_idx.get(b, 0))\n",
    "X_brand_test = df_test.brand_name.apply(lambda b: brands_idx.get(b, 0))\n",
    "\n",
    "X_brand = X_brand.values.reshape(-1, 1) \n",
    "X_brand_valid = X_brand_valid.values.reshape(-1, 1) \n",
    "X_brand_test = X_brand_test.values.reshape(-1, 1)\n",
    "\n",
    "brand_voc_size = len(brands) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing other features...\n"
     ]
    }
   ],
   "source": [
    "print('processing other features...')\n",
    "\n",
    "X_item_cond = (df_train.item_condition_id - 1).astype('uint8').values.reshape(-1, 1)\n",
    "X_item_cond_valid = (df_valid.item_condition_id - 1).astype('uint8').values.reshape(-1, 1)\n",
    "X_item_cond_test = (df_test.item_condition_id - 1).astype('uint8').values.reshape(-1, 1)\n",
    "\n",
    "X_shipping = df_train.shipping.astype('float32').values.reshape(-1, 1)\n",
    "X_shipping_valid = df_valid.shipping.astype('float32').values.reshape(-1, 1)\n",
    "X_shipping_test = df_test.shipping.astype('float32').values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(inputs, num_filters, filter_size, padding='same'):\n",
    "    he_std = np.sqrt(2 / (filter_size * num_filters))\n",
    "    out = tf.layers.conv1d(\n",
    "        inputs=inputs, filters=num_filters, padding=padding,\n",
    "        kernel_size=filter_size,\n",
    "        activation=tf.nn.relu, \n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=he_std))\n",
    "    return out\n",
    "\n",
    "def dense(X, size, reg=0.0, activation=None):\n",
    "    he_std = np.sqrt(2 / int(X.shape[1]))\n",
    "    out = tf.layers.dense(X, units=size, activation=activation, \n",
    "                     kernel_initializer=tf.random_normal_initializer(stddev=he_std),\n",
    "                     kernel_regularizer=tf.contrib.layers.l2_regularizer(reg))\n",
    "    return out\n",
    "\n",
    "def embed(inputs, size, dim):\n",
    "    \"\"\"输出[sie, dim]中经过inputs过滤的embedding向量\"\"\"\n",
    "    std = np.sqrt(2 / dim)\n",
    "    emb = tf.Variable(tf.random_uniform([size, dim], -std, std))\n",
    "    lookup = tf.nn.embedding_lookup(emb, inputs)\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_embeddings_dim = 32\n",
    "name_seq_len = X_name.shape[1]\n",
    "\n",
    "desc_embeddings_dim = 32\n",
    "desc_seq_len = X_desc.shape[1]\n",
    "\n",
    "brand_embeddings_dim = 4\n",
    "\n",
    "cat_embeddings_dim = 12\n",
    "cat_seq_len = X_cat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name.shape is (?, 170)\n",
      "desc.shape is (?, 400)\n",
      "brand.shape is (?, 4)\n",
      "cat.shape is (?, 12)\n",
      "concatenated dim: (?, 592)\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "graph.seed = 1\n",
    "\n",
    "with graph.as_default():\n",
    "    place_name = tf.placeholder(tf.int32, shape=(None, name_seq_len))\n",
    "    place_desc = tf.placeholder(tf.int32, shape=(None, desc_seq_len))\n",
    "    place_brand = tf.placeholder(tf.int32, shape=(None, 1))\n",
    "    place_cat = tf.placeholder(tf.int32, shape=(None, cat_seq_len))\n",
    "    place_ship = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    place_cond = tf.placeholder(tf.uint8, shape=(None, 1))\n",
    "    place_y = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "    place_lr = tf.placeholder(tf.float32, shape=(), )\n",
    "    \n",
    "    # title\n",
    "    name = embed(place_name, name_voc_size, name_embeddings_dim)\n",
    "    name = conv1d(name, num_filters=10, filter_size=3)\n",
    "    name = tf.nn.dropout(name, keep_prob=0.5)\n",
    "    name = tf.layers.flatten(name)\n",
    "    tf.summary.histogram(\"name\", name)\n",
    "    print(\"name.shape is {}\".format(name.shape))\n",
    "    \n",
    "    # description \n",
    "    desc = embed(place_desc, desc_voc_size, desc_embeddings_dim)\n",
    "    desc = conv1d(desc, num_filters=10, filter_size=3)\n",
    "    desc = tf.nn.dropout(desc, keep_prob=0.5)\n",
    "    desc = tf.layers.flatten(desc)\n",
    "    tf.summary.histogram(\"desc\", desc)\n",
    "    print(\"desc.shape is {}\".format(desc.shape))\n",
    "    \n",
    "    # brand\n",
    "    brand = embed(place_brand, brand_voc_size, brand_embeddings_dim)\n",
    "    brand = tf.layers.flatten(brand)\n",
    "    tf.summary.histogram(\"brand\", brand)\n",
    "    print(\"brand.shape is {}\".format(brand.shape))\n",
    "    \n",
    "    # category \n",
    "    cat = embed(place_cat, cat_voc_size, cat_embeddings_dim)\n",
    "    cat = tf.layers.average_pooling1d(cat, pool_size=cat_seq_len, strides=1, padding='valid')\n",
    "    cat = tf.layers.flatten(cat)\n",
    "    tf.summary.histogram(\"cat\", cat)\n",
    "    print(\"cat.shape is {}\".format(cat.shape))\n",
    "    \n",
    "    # ship\n",
    "    ship = place_ship\n",
    "\n",
    "    # condition\n",
    "    cond = tf.one_hot(place_cond, 5)\n",
    "    cond = tf.layers.flatten(cond)\n",
    "\n",
    "    out = tf.concat([name, desc, brand, cat, ship, cond], axis=1)\n",
    "    print('concatenated dim:', out.shape)\n",
    "    \n",
    "    out = dense(out, size=100, activation=None)\n",
    "    out = tf.nn.dropout(out, keep_prob=0.5)\n",
    "    out = dense(out, size=1)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(place_y, out)\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    rmse = tf.sqrt(loss)\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=place_lr).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    merged = tf.summary.merge_all() # merge_all需要在graph的定义中声明，否则无效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batches(inputs, batch_size):\n",
    "    \"\"\"提供batch_size数据\"\"\"\n",
    "    n = len(inputs)\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield inputs[i: i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 300, loss: 0.6367616057395935\n",
      "iter: 600, loss: 0.5309112071990967\n",
      "iter: 900, loss: 0.4952414035797119\n",
      "iter: 1200, loss: 0.47608503699302673\n",
      "iter: 1500, loss: 0.4624232053756714\n",
      "iter: 1800, loss: 0.45448222756385803\n",
      "iter: 2100, loss: 0.44892504811286926\n",
      "iter: 2400, loss: 0.4470916986465454\n",
      "iter: 2700, loss: 0.441667765378952\n",
      "iter: 3000, loss: 0.438157320022583\n",
      "iter: 3300, loss: 0.4363723695278168\n",
      "iter: 3600, loss: 0.433505654335022\n",
      "iter: 3900, loss: 0.43136122822761536\n",
      "iter: 4200, loss: 0.4304935932159424\n",
      "iter: 4500, loss: 0.43019935488700867\n",
      "iter: 4800, loss: 0.4289141297340393\n",
      "iter: 5100, loss: 0.4275100827217102\n",
      "iter: 5400, loss: 0.4270131587982178\n",
      "iter: 5700, loss: 0.42648443579673767\n",
      "iter: 6000, loss: 0.42400550842285156\n",
      "iter: 6300, loss: 0.4231116473674774\n",
      "iter: 6600, loss: 0.42257654666900635\n",
      "iter: 6900, loss: 0.421375572681427\n",
      "iter: 7200, loss: 0.4211971163749695\n",
      "iter: 7500, loss: 0.4205610752105713\n",
      "iter: 7800, loss: 0.4205142855644226\n",
      "iter: 8100, loss: 0.42000412940979004\n",
      "iter: 8400, loss: 0.4206872284412384\n",
      "iter: 8700, loss: 0.41974830627441406\n",
      "iter: 9000, loss: 0.4190821349620819\n",
      "iter: 9300, loss: 0.4200149178504944\n",
      "iter: 9600, loss: 0.4198896586894989\n",
      "iter: 9900, loss: 0.4179559350013733\n",
      "iter: 10200, loss: 0.4183909296989441\n",
      "iter: 10500, loss: 0.4187319874763489\n",
      "iter: 10800, loss: 0.4186955690383911\n",
      "iter: 11100, loss: 0.4192839562892914\n",
      "iter: 11400, loss: 0.42015373706817627\n",
      "iter: 11700, loss: 0.4194108545780182\n",
      "iter: 12000, loss: 0.4181990325450897\n",
      "iter: 12300, loss: 0.41886627674102783\n",
      "iter: 12600, loss: 0.4185624122619629\n",
      "iter: 12900, loss: 0.4183974266052246\n",
      "iter: 13200, loss: 0.4180312752723694\n",
      "iter: 13500, loss: 0.4180671274662018\n",
      "iter: 13800, loss: 0.41823339462280273\n",
      "iter: 14100, loss: 0.41686853766441345\n",
      "iter: 14400, loss: 0.4184507727622986\n",
      "iter: 14700, loss: 0.4184034466743469\n",
      "iter: 15000, loss: 0.41785940527915955\n",
      "iter: 15300, loss: 0.41804444789886475\n",
      "iter: 15600, loss: 0.4175117611885071\n",
      "iter: 15900, loss: 0.4177335202693939\n",
      "iter: 16200, loss: 0.4186151623725891\n",
      "iter: 16500, loss: 0.4175860285758972\n",
      "iter: 16800, loss: 0.4164279103279114\n",
      "iter: 17100, loss: 0.4174802899360657\n",
      "iter: 17400, loss: 0.418230801820755\n",
      "iter: 17700, loss: 0.41774994134902954\n",
      "iter: 18000, loss: 0.41714802384376526\n",
      "iter: 18300, loss: 0.41700389981269836\n",
      "iter: 18600, loss: 0.4166889786720276\n",
      "iter: 18900, loss: 0.4170691967010498\n",
      "iter: 19200, loss: 0.41653528809547424\n",
      "iter: 19500, loss: 0.41761699318885803\n",
      "iter: 19800, loss: 0.41721758246421814\n",
      "iter: 20100, loss: 0.4171561598777771\n",
      "iter: 20400, loss: 0.41729140281677246\n",
      "iter: 20700, loss: 0.4175756573677063\n",
      "iter: 21000, loss: 0.4175608456134796\n",
      "iter: 21300, loss: 0.4165343940258026\n",
      "iter: 21600, loss: 0.4174373149871826\n",
      "iter: 21900, loss: 0.41687923669815063\n",
      "iter: 22200, loss: 0.4165598452091217\n",
      "iter: 22500, loss: 0.41699448227882385\n",
      "iter: 22800, loss: 0.4176180958747864\n",
      "iter: 23100, loss: 0.4170144498348236\n",
      "iter: 23400, loss: 0.41701480746269226\n",
      "iter: 23700, loss: 0.41756322979927063\n",
      "iter: 24000, loss: 0.41678327322006226\n",
      "iter: 24300, loss: 0.4170409142971039\n",
      "iter: 24600, loss: 0.41718217730522156\n",
      "iter: 24900, loss: 0.4172896146774292\n",
      "iter: 25200, loss: 0.4164820909500122\n",
      "iter: 25500, loss: 0.4168913662433624\n",
      "iter: 25800, loss: 0.41678714752197266\n",
      "iter: 26100, loss: 0.4168893098831177\n",
      "iter: 26400, loss: 0.41678401827812195\n",
      "iter: 26700, loss: 0.4156482219696045\n",
      "iter: 27000, loss: 0.4167764186859131\n",
      "iter: 27300, loss: 0.4163720905780792\n",
      "iter: 27600, loss: 0.4166218042373657\n",
      "iter: 27900, loss: 0.41706582903862\n",
      "iter: 28200, loss: 0.4168124794960022\n",
      "iter: 28500, loss: 0.416873961687088\n",
      "iter: 28800, loss: 0.41624122858047485\n",
      "iter: 29100, loss: 0.41738617420196533\n",
      "iter: 29400, loss: 0.416043221950531\n",
      "iter: 29700, loss: 0.41648977994918823\n",
      "iter: 30000, loss: 0.41571420431137085\n",
      "iter: 30300, loss: 0.41638314723968506\n",
      "iter: 30600, loss: 0.41729721426963806\n",
      "iter: 30900, loss: 0.4176647663116455\n",
      "iter: 31200, loss: 0.41637614369392395\n",
      "iter: 31500, loss: 0.41554370522499084\n",
      "iter: 31800, loss: 0.4172588586807251\n",
      "iter: 32100, loss: 0.41617894172668457\n",
      "iter: 32400, loss: 0.41596147418022156\n",
      "iter: 32700, loss: 0.4163869023323059\n",
      "iter: 33000, loss: 0.41618630290031433\n",
      "iter: 33300, loss: 0.4158909320831299\n",
      "iter: 33600, loss: 0.41611677408218384\n",
      "iter: 33900, loss: 0.41619160771369934\n",
      "iter: 34200, loss: 0.41660788655281067\n",
      "iter: 34500, loss: 0.4172365069389343\n",
      "iter: 34800, loss: 0.416788786649704\n",
      "iter: 35100, loss: 0.4163333475589752\n",
      "iter: 35400, loss: 0.4162279963493347\n",
      "iter: 35700, loss: 0.4157732129096985\n",
      "iter: 36000, loss: 0.4162307679653168\n",
      "iter: 36300, loss: 0.41663119196891785\n",
      "iter: 36600, loss: 0.41624316573143005\n",
      "iter: 36900, loss: 0.4162781238555908\n",
      "iter: 37200, loss: 0.41686558723449707\n",
      "iter: 37500, loss: 0.4163896441459656\n",
      "iter: 37800, loss: 0.4163321554660797\n",
      "iter: 38100, loss: 0.4166560769081116\n",
      "iter: 38400, loss: 0.4171384274959564\n",
      "iter: 38700, loss: 0.4159046709537506\n",
      "iter: 39000, loss: 0.41518938541412354\n",
      "iter: 39300, loss: 0.41683414578437805\n",
      "iter: 39600, loss: 0.4155651330947876\n",
      "iter: 39900, loss: 0.4164089560508728\n",
      "iter: 40200, loss: 0.4161092936992645\n",
      "iter: 40500, loss: 0.4169567823410034\n",
      "iter: 40800, loss: 0.4157225489616394\n",
      "iter: 41100, loss: 0.4162442088127136\n",
      "iter: 41400, loss: 0.41495734453201294\n"
     ]
    }
   ],
   "source": [
    "writer = tf.summary.FileWriter(\"loss_log/\", session.graph)\n",
    "session = tf.Session(graph=graph)\n",
    "session.run(init)\n",
    "\n",
    "count = 0\n",
    "for i in range(20):\n",
    "    np.random.seed(i)\n",
    "    train_indices = list(range(len(X_name)))\n",
    "    np.random.shuffle(train_indices)\n",
    "    \n",
    "    # 控制lr \n",
    "    if i <= 2:\n",
    "        lr = 0.001\n",
    "    elif i <= 10:\n",
    "        lr = 0.0001\n",
    "    else:\n",
    "        lr = 0.00005\n",
    "    \n",
    "    for idx in prepare_batches(train_indices, 500):\n",
    "        feed_dict_op = {\n",
    "            place_name: X_name[idx],\n",
    "            place_desc: X_desc[idx],\n",
    "            place_brand: X_brand[idx],\n",
    "            place_cat: X_cat[idx],\n",
    "            place_ship: X_shipping[idx],\n",
    "            place_cond: X_item_cond[idx],\n",
    "            place_y: y_train[idx],\n",
    "            place_lr: lr\n",
    "        }\n",
    "        \n",
    "        _ = session.run(train_step, feed_dict=feed_dict_op)\n",
    "        \n",
    "        count += 1\n",
    "        if count % 300 == 0:\n",
    "            feed_dict_op = {\n",
    "                place_name: X_name_valid,\n",
    "                place_desc: X_desc_valid,\n",
    "                place_brand: X_brand_valid,\n",
    "                place_cat: X_cat_valid,\n",
    "                place_ship: X_shipping_valid,\n",
    "                place_cond: X_item_cond_valid,\n",
    "                place_y: y_valid\n",
    "            }\n",
    "            loss_op, merged_op = session.run([loss, merged], feed_dict_op)\n",
    "            print(\"iter: {}, loss: {}\".format(count, loss_op))\n",
    "            writer.add_summary(merged_op, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
